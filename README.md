## My website https://www.imomer.net
## t5 mental assisstant ai model and api

## api Documentation
root : http://api.imomer.net/ai/
 
endpoint /assistant/

 method : POST
 
 data : {'message': 'your message'}

 # example 1
```python
import requests

endpoint = 'http://api.imomer.net/ai/assistant/'
resp = requests.post(endpoint,data={'message':'i have problems with my sleep'})

print(resp.json())

```
```python
response:
{'mental_assistant': "Sleep problems can be caused by a variety of factors, including stress, anxiety, and medical conditions. Let's explore your sleep habits and see if there are any underlying causes. Have you considered seeking professional help or seeking out a sleep specialist?", 'NOTE': 'This model is currently in its early access phase and is actively undergoing training and development. As a result, it may not provide fully accurate or complete responses. Please exercise caution and consider the information generated by this model as provisional. We appreciate your patience and feedback as we work to improve its capabilities.'}
```

# example 2
```python
import requests

endpoint = 'http://api.imomer.net/ai/assistant/'
resp = requests.post(endpoint,data={'message':'i feel alone all the time and tired after studying for long time'})

print(resp.json())
```

```python
response:
{'mental_assistant': "Hello, there are many reasons why you feel alone, and many people feel less fatigued after studying for long time. First, let's explore why you feel alone after studying for long time and work on developing strategies to improve your self-esteem.", 'NOTE': 'This model is currently in its early access phase and is actively undergoing training and development. As a result, it may not provide fully accurate or complete responses. Please exercise caution and consider the information generated by this model as provisional. We appreciate your patience and feedback as we work to improve its capabilities.'}
```
## use the ai model localy on your device 
### Import libraries
```python
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM
```
We import TensorFlow and the Transformer libraries.
Load model and tokenizer
```python
tokenizer = AutoTokenizer.from_pretrained('mental_tokeniser')
model = TFAutoModelForSeq2SeqLM.from_pretrained('t5_mental_assisstant_v5')
```
We load the pretrained tokenizer and T5 model from disk.
Chat loop
```python

while True:

  inputs = input('>>>>')
  
  inputs = tokenizer([inputs], max_length=128, padding=True, truncation=True, return_tensors='tf')
  ```
We create a loop to repeatedly get user input and tokenize it.
The tokenizer prepares the input for the model, padding and truncating as needed.
Generate response
```python

  output = model.generate(inputs.input_ids, max_length=128)
  
  response = tokenizer.decode(output[0], skip_special_tokens=True)
  ```
We pass the tokenized input to the model to generate a sequence.
The tokenizer decodes the output tokens into a readable string.
Print response
```pytho

  print('doctor >', response)
````
